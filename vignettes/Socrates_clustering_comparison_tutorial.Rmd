---
title: "RRscca Tutorial: Comparing Normalization Methods"
author: "Alexandre P. Marand"
date: "10/27/2020"
output:
    html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load RRscca and raw data

Here, we demonstrate how to load the following inputs to create an `RRscca` object:

1. **binary sparse peak x cell matrix** (a gzipped text file in triplet tsv format).
2. **meta-data** saved as a tsv document containing various per-cell metrics. 

Meta-data is not necessarily required, but useful for evaluating technical effects in clustering, as well as other downstream steps. An example of input data formats for 1,500 PBMC cells and how to construct an `RRscca` object from scratch is shown below. 

```{r loadRaw, warnings=FALSE}
# load library
suppressWarnings(library("RRscca"))

# specify paths to raw data in RRscca package
input <- system.file("extdata", "pbmc_atac_10x.1.5K_cells.sparse.gz", package="RRscca")
meta <- system.file("extdata", "pbmc_atac_10x.1.5K_cells.metadata.txt", package="RRscca")

# load raw data for viewing
input.format <- read.table(input)
meta.format <- read.table(meta)

# view
head(input.format)
head(meta.format)

# load data into RRscca object
obj <- loadSparseData(input=input, meta=meta, verbose=T)
```

***


## Filter peaks and cells

To reduce the effects of outlier peaks and cells on clustering, it is often helpful to remove cells with few accessible peaks, and peaks with extreme accessibility profiles (i.e. peaks that are accessible in all, or very few cells). Specifically, the cell x peak matrix can be filtered by adjusting heurstic frequency thresholds after visual inspection of cell and peak accessibility distributions. Let's investigate these distributions further to determine reasonable thresholds for this particular data set.

```{r view_distributions, fig.height=6, fig.width=14}
# estimate log10 number of accessible regions per cell
cell.counts <- Matrix::colSums(obj$counts)

# estimate peak accessibility frequency across cells
site.freq <- Matrix::rowMeans(obj$counts)

# plot distributions
layout(matrix(c(1:2), ncol=2))
par(mar=c(3,3,1,1))
plot(density(cell.counts), main="log10 cell counts", log="x")
abline(v=1000, col="red")
plot(density(site.freq), main="peak accessibility frequency", log="x")
```

It appears that most cells have a median over around 7,000 accessible peaks. The cells on the lower tail of the distribution may reflect broken nuclei, so we'll remove cells with less than 1,000 open peaks from the analysis. The distribution of average peak accessibilities doesnt show any clear (lower-tail) cut-offs, therefore, we will use the default thresholds (`min.t=0.001, max.t=0.005`). The arguments `min.t` and `max.t` set the minimum peak accessibility frequency and upper (99.5%) quantile cut-offs, respectively.

```{r filter_data}
# filter matrix 
obj <- cleanData(obj, min.c=1000, min.t=0.001, max.t=0.005, verbose=T)
```


#### Filtering peaks enriched in pre-computed clusters
**NOTE** If users have generated crude clusters, such as *in silico* sorting described by [Cusanovich et al. 2018](https://linkinghub.elsevier.com/retrieve/pii/S0092-8674(18)30855-9), the parameters `min.p` and `preclusterID` allow users to set minimum peak accessibility frequencies for pre-specied groups. Below, we constrain peaks to be accessible in at least 5% of cells in at least one `crude_cluster`. See `?cleanData` for more details.

```{r filter_data_clusters, eval=FALSE}
# simulate 10 random clusters, 
obj$meta$crude_clusters <- sample(factor(seq(1:10)), nrow(obj$meta), replace=T)
obj <- cleanData(obj, min.p=0.05, preclusterID="crude_clusters")
```

***


## Normalization

With a filtered cell x peak matrix in hand, we can now calculate normalized accessibility scores (Pearson's residuals) across all cells and peaks using regularized quasibinomial logistic regression. Note that the function `regModel` can be parallelized by setting `nthreads` to a number greater than 1. Parallel implementations depend on the `doSNOW` library. In the example below, we set the number of threads to 4 to speed-up the analysis. In addition to regularized modeling, we provide additional functions to normalize chromatin accessibility profiles. Currently included in this release (supplement to `regModel`) are `tfidf`, `logisticModel`, and `regModel2`. In all cases, we should change the residual slot name from the default to avoid overwriting after each call (accomplished by specifying unique `slotName`). If changing from default, make sure to specify the updated slot names in subsequent analyses. Slot names should be different for each step, such as different slot names between `regModel` and `reduceDims`. If only using a single normalization method, leave the default naming, it will avoid potentially overwriting various data sets. Running multiple normalization procedures in parallel should only be done once as a way to inform the method providing the most desirable characteristics. We will run multiple normalization schemes to compare their performance. Each scheme is saved to a different slot, in parallel, for each step. **PAY CAREFUL ATTENTION TO ARGUMENTS DEALING WITH SLOTNAMES**.

#### Regularized logistic regression

The recommended normalization procedure.

```{r regModel_1, eval=TRUE}
# run regularized regression
obj <- regModel(obj, verbose=F, nthreads=4, slotName="NORM1")
```

#### TF-IDF

`tfidf` has the benefit of keeping the normalized data in a sparse format that allows users to conserve and reduce memory usage. We will compare TF-IDF normalization with quasibinomial later on in this tutorial. The TF-IDF function was adapted from Andrew Hill, the original implementation can be found [here](http://andrewjohnhill.com/blog/2019/05/06/dimensionality-reduction-for-scatac-data/)

```{r tfidf, eval=TRUE}
# run TF-IDF normalization
obj <- tfidf(obj, slotName="NORM2")
```    

#### Logistic regression without regularization

`logisticModel` does not regularize parameters by explicitly learning a model each peak independently. To run logistic regression with 4 threads without any regularization, run the following line of code. See `?logisticModel` for more details.

```{r logisticModel, eval=TRUE}
# run logisticModel
obj <- logisticModel(obj, verbose=T, nthreads=4, slotName="NORM3")
```

#### Regularized logistic regression with cell and peak sub-sampling.

As the number of cells can quickly become prohibitive (we have tested up to 60K cells across 160K peaks, requiring upwards of 50G memory), we extended the peak sub-sampling procedure for sampling cells. The function `regModel2` samples subsets of cells uniforming for factors specified in a column from the meta data, such as sampling 1,000 cells from different biological replicates or tissues. Sampling down to around 1,000 cells dramatically speeds up the computation with little effects on clustering. Here, we will sample 750 cells (half of the total 1,500) for building the models. See `?regModel2` for more details.

```{r regModel2, eval=TRUE}
# run regularized model with cell sampling
obj$meta$library <- factor(1)
obj <- regModel2(obj, subcells=750, verbose=T, slotName="NORM4")
```

***


## Reducing dimensions

### Singular Value Decomposition (SVD)

After normalizing peak x cell chromatin accessibility profiles using one of the aforementioned methods, we can reduce the dimensionality of residual matrix to remove noise and better model cell-cell relationships in a reduced space. Dimensionality reduction is implemented via Singular Value Decomposition (SVD) from the `irlba` package. We will reduce the dimensions of all the normalization methods by specifically selecting the slots from each approach one-by-one.

```{r reduceDims}
# reduce dimensionality of the residual matrix
obj <- reduceDims(obj, n.pcs=50, cor.max=0.7, verbose=T, residuals_slotName="NORM1", svd_slotName="SVD1")
obj <- reduceDims(obj, n.pcs=50, cor.max=0.7, verbose=T, residuals_slotName="NORM2", svd_slotName="SVD2")
obj <- reduceDims(obj, n.pcs=50, cor.max=0.7, verbose=T, residuals_slotName="NORM3", svd_slotName="SVD3")
obj <- reduceDims(obj, n.pcs=50, cor.max=0.7, verbose=T, residuals_slotName="NORM4", svd_slotName="SVD4")
```

The above commands estimate the first 50 singular values, and removes singular values correlated with technical variation (read depth) above a Spearman Correlation Coefficient of 0.7. 

### Projecting into a reduced dimensionality with UMAP

Similarity between cells is most easily visualized on two dimensions. Uniform Manifold Approximation Projection (UMAP) has gained popularity in single-cell approaches owing to its scalability and capacity for maintaining both global and local data structure, greatly aiding data interpretations. To generate UMAP embeddings, we can run the `projectUMAP` function which relies on `uwot::umap`:

```{r projectUMAP}
# run projectUMAP
obj <- projectUMAP(obj, svd_slotName="SVD1", umap_slotName="UMAP1",verbose=T)
obj <- projectUMAP(obj, svd_slotName="SVD2", umap_slotName="UMAP2",verbose=T)
obj <- projectUMAP(obj, svd_slotName="SVD3", umap_slotName="UMAP3",verbose=T)
obj <- projectUMAP(obj, svd_slotName="SVD4", umap_slotName="UMAP4",verbose=T)
```

We can quickly visualize the reduced embedding. As you can see below, they are quite similar. In cases where speed and memory usage are central factors, it may be more advisable to use TF-IDF normalization in place of RRscca. One benefit of using model-based approaches is that the normalized values can be readily interpretted for down-stream analyses. 

```{r plotUMAPraw, fig.height=10, fig.width=10}
# plot UMAP results
layout(matrix(c(1:4), ncol=2, byrow=T))
par(mar=c(4,4,1,1))
plot(obj$UMAP1, pch=16, cex=0.2, main="regModel")
plot(obj$UMAP2, pch=16, cex=0.2, main="tfidf")
plot(obj$UMAP3, pch=16, cex=0.2, main="LogisticModel")
plot(obj$UMAP4, pch=16, cex=0.2, main="regModel2")
```

***


## Graph-based clustering

Visualization of the UMAP embeddings suggests several groups of cells with distinct identities. We can cluster cells into groups using graph-based clustering by leaning on Louvain and Leiden clustering approaches provided by the popular `Seurat` package. Below is a wrapper for running graph-based clustering in the SVD space via Seurat. Cluster membership is appended to the meta data.frame under the column 'LouvainClusters' by default. For a list of tuneable parameters, run `?callClusters` in the R console. 

```{r callClusters}
# run clustering
obj <- callClusters(obj, svd_slotName="SVD1", umap_slotName="UMAP1", cluster_slotName="Clusters1", verbose=T)
obj <- callClusters(obj, svd_slotName="SVD2", umap_slotName="UMAP2", cluster_slotName="Clusters2", verbose=T)
obj <- callClusters(obj, svd_slotName="SVD3", umap_slotName="UMAP3", cluster_slotName="Clusters3", verbose=T)
obj <- callClusters(obj, svd_slotName="SVD4", umap_slotName="UMAP4", cluster_slotName="Clusters4", verbose=T)
```

## Plotting results

Having run the clustering algorithm, we can now visualize the different groupings on the UMAP embedding. 

```{r plotClusters, fig.height=10, fig.width=10}
layout(matrix(c(1:4), ncol=2, byrow=T))
par(mar=c(4,4,1,1))
plotUMAP(obj, cluster_slotName="Clusters1", main="regModel")
plotUMAP(obj, cluster_slotName="Clusters2", main="tfidf")
plotUMAP(obj, cluster_slotName="Clusters3", main="logisticModel")
plotUMAP(obj, cluster_slotName="Clusters4", main="regModel2")
```

## Saving results

The `RRscca` object is updated iteratively after each processing step. To save results for sharing or future exploration, you can use the following command to save a snapshot of current stage of analysis:

```{r savingData, eval=FALSE}
saveRDS(obj, file="RRscca_object.rds")
```

## Accessing results

Results are appended to the `RRscca` object after each function, as described above. Below describes the location of different data sets up to this point.

* Raw counts 
    + `obj$counts`
    + generated by `loadSparseData`
* Raw meta-data  
    + `obj$meta`
    + generated by `loadSparseData`
* SVD/PCA
    + `obj$PCA` (Default, can be customized)
    + generated by `reduceDims`
* Initial UMAP
    + `obj$UMAP` (Default, can be customized)
    + generated by `projectUMAP`
* Cluster + meta-data (filtered cells)
    + `obj$Clusters` (Default, can be customized)
    + generated by `callClusters`

***


### Session Information

```{r sessionInfo}
sessionInfo()
```
